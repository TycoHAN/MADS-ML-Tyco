{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60693c86",
   "metadata": {},
   "source": [
    "Add libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c06153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "import torch.optim as optim\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "from tomlserializer import TOMLSerializer\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59781ae2",
   "metadata": {},
   "source": [
    "Prepare train and test datastreamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6bb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-05 08:44:20.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\tycoh\\.cache\\mads_datasets\\fashionmnist\u001b[0m\n",
      "\u001b[32m2025-10-05 08:44:20.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at C:\\Users\\tycoh\\.cache\\mads_datasets\\fashionmnist\\fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "batchsize = 64 \n",
    "streamers = fashionfactory.create_datastreamer(batchsize=batchsize, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3177821",
   "metadata": {},
   "source": [
    "Performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb0ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d02df",
   "metadata": {},
   "source": [
    "Define architecture network, we want to experiment with extra convolution layers, drop out layers and normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb296d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.dropout = dropout\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout), # adding dropout layer\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cfe3f",
   "metadata": {},
   "source": [
    "Adding ml flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a2b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = Path(\"models\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir()\n",
    "    print(f\"Created {modeldir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a02f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'adding_dropout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "396ba232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:c:/Users/tycoh/Desktop/MADS-ML-Tyco/2-hypertuning-mlflow/mlruns/2', creation_time=1759646653266, experiment_id='2', last_update_time=1759646653266, lifecycle_stage='active', name='adding_dropout', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbf04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-05 08:46:25.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20251005-084625\u001b[0m\n",
      "\u001b[32m2025-10-05 08:46:25.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:01<00:00, 101.85it/s]\n",
      "\u001b[32m2025-10-05 08:46:26.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 1.1877 test 0.7636 metric ['0.7070']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 134.13it/s]\n",
      "\u001b[32m2025-10-05 08:46:28.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.6976 test 0.6273 metric ['0.7737']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:01<00:00, 97.62it/s]\n",
      "\u001b[32m2025-10-05 08:46:30.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5981 test 0.5491 metric ['0.8096']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:05<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "units1 = 64\n",
    "units2 = 64\n",
    "dropout = 0.2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = NeuralNetwork(num_classes=10, units1=units1, units2=units2, dropout=dropout)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    settings = TrainerSettings(\n",
    "        epochs=3,\n",
    "        metrics=[accuracy],\n",
    "        logdir=\"modellogs\",\n",
    "        train_steps=128,\n",
    "        valid_steps=128,\n",
    "        reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],\n",
    "    )\n",
    "    mlflow.log_params({\n",
    "                \"epochs\": settings.epochs,\n",
    "                \"train_steps\": settings.train_steps,\n",
    "                \"valid_steps\": settings.valid_steps,\n",
    "                \"units1\": model.units1,\n",
    "                \"units2\": model.units2,\n",
    "                \"batchsize\": batchsize,\n",
    "                \"logdir\": settings.logdir,\n",
    "                \"reporttypes\": [r.name for r in settings.reporttypes],  \n",
    "            })\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        settings=settings,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        traindataloader=trainstreamer,\n",
    "        validdataloader=validstreamer,\n",
    "        scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "    \n",
    "    trainer.loop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a04108",
   "metadata": {},
   "source": [
    "Try adding convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b5104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, filters, units1, units2, input_size=(32, 1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.in_channels = input_size[1]\n",
    "        self.input_size = input_size\n",
    "        self.filters = filters\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        activation_map_size = self._conv_test(input_size)\n",
    "        logger.info(f\"Aggregating activationmap with size {activation_map_size}\")\n",
    "        self.agg = nn.AvgPool2d(activation_map_size)\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(filters, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, 10),\n",
    "        )\n",
    "\n",
    "    def _conv_test(self, input_size=(32, 1, 28, 28)):\n",
    "        x = torch.ones(input_size)\n",
    "        x = self.convolutions(x)\n",
    "        return x.shape[-2:]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = self.agg(x)\n",
    "        logits = self.dense(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83317c",
   "metadata": {},
   "source": [
    "Setup loss function, train settings, train logging (MLFLOW), grid search units,  and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "201942ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = Path(\"models\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir()\n",
    "    print(f\"Created {modeldir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63df83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'adding_dropout_batchnorm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbdf222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/26 14:32:13 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/26 14:32:13 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "INFO  [alembic.runtime.migration] Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "INFO  [alembic.runtime.migration] Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/09/26 14:32:15 INFO mlflow.tracking.fluent: Experiment with name 'adding_dropout_batchnorm' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:c:/Users/tycoh/Desktop/MADS-ML-Tyco/2-hypertuning-mlflow/mlruns/1', creation_time=1758889935286, experiment_id='1', last_update_time=1758889935286, lifecycle_stage='active', name='adding_dropout_batchnorm', tags={}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad507949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-26 15:28:59.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250926-152859\u001b[0m\n",
      "\u001b[32m2025-09-26 15:28:59.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 128.47it/s]\n",
      "\u001b[32m2025-09-26 15:29:00.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.9076 test 0.6129 metric ['0.7755']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 147.57it/s]\n",
      "\u001b[32m2025-09-26 15:29:02.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.5490 test 0.5435 metric ['0.7985']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:01<00:00, 107.22it/s]\n",
      "\u001b[32m2025-09-26 15:29:03.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5090 test 0.4788 metric ['0.8279']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 132.46it/s]\n",
      "\u001b[32m2025-09-26 15:29:05.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.4683 test 0.4572 metric ['0.8336']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:01<00:00, 100.39it/s]\n",
      "\u001b[32m2025-09-26 15:29:07.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.4307 test 0.4482 metric ['0.8417']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:08<00:00,  1.61s/it]\n",
      "\u001b[32m2025-09-26 15:29:07.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250926-152907\u001b[0m\n",
      "\u001b[32m2025-09-26 15:29:07.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 191.52it/s]\n",
      "\u001b[32m2025-09-26 15:29:08.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 1.0142 test 0.6624 metric ['0.7567']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 199.86it/s]\n",
      "\u001b[32m2025-09-26 15:29:09.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.6171 test 0.5542 metric ['0.8044']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 194.56it/s]\n",
      "\u001b[32m2025-09-26 15:29:10.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5159 test 0.5019 metric ['0.8208']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 165.56it/s]\n",
      "\u001b[32m2025-09-26 15:29:11.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.5059 test 0.4912 metric ['0.8300']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 170.32it/s]\n",
      "\u001b[32m2025-09-26 15:29:12.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.4769 test 0.4777 metric ['0.8247']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:05<00:00,  1.11s/it]\n",
      "\u001b[32m2025-09-26 15:29:12.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250926-152912\u001b[0m\n",
      "\u001b[32m2025-09-26 15:29:12.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 149.07it/s]\n",
      "\u001b[32m2025-09-26 15:29:14.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.9498 test 0.6402 metric ['0.7626']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 133.60it/s]\n",
      "\u001b[32m2025-09-26 15:29:15.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.5823 test 0.5341 metric ['0.8040']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 148.61it/s]\n",
      "\u001b[32m2025-09-26 15:29:17.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5073 test 0.4936 metric ['0.8275']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 128.51it/s]\n",
      "\u001b[32m2025-09-26 15:29:18.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.4812 test 0.4895 metric ['0.8253']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:01<00:00, 107.90it/s]\n",
      "\u001b[32m2025-09-26 15:29:20.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.4522 test 0.4469 metric ['0.8417']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:07<00:00,  1.51s/it]\n",
      "\u001b[32m2025-09-26 15:29:20.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250926-152920\u001b[0m\n",
      "\u001b[32m2025-09-26 15:29:20.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 157.70it/s]\n",
      "\u001b[32m2025-09-26 15:29:21.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.9520 test 0.6324 metric ['0.7748']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 157.36it/s]\n",
      "\u001b[32m2025-09-26 15:29:22.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.5865 test 0.5696 metric ['0.7971']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 195.16it/s]\n",
      "\u001b[32m2025-09-26 15:29:23.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5158 test 0.4827 metric ['0.8281']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 192.31it/s]\n",
      "\u001b[32m2025-09-26 15:29:24.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.4867 test 0.5091 metric ['0.8188']\u001b[0m\n",
      "\u001b[32m2025-09-26 15:29:24.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.4827, current loss 0.5091.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 149.04it/s]\n",
      "\u001b[32m2025-09-26 15:29:26.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.4551 test 0.4499 metric ['0.8391']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:05<00:00,  1.16s/it]\n",
      "\u001b[32m2025-09-26 15:29:26.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250926-152926\u001b[0m\n",
      "\u001b[32m2025-09-26 15:29:26.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 187.25it/s]\n",
      "\u001b[32m2025-09-26 15:29:27.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 1.0103 test 0.6459 metric ['0.7632']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 169.87it/s]\n",
      "\u001b[32m2025-09-26 15:29:28.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.6121 test 0.5395 metric ['0.8077']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 151.65it/s]\n",
      "\u001b[32m2025-09-26 15:29:30.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5466 test 0.5071 metric ['0.8201']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 144.77it/s]\n",
      "\u001b[32m2025-09-26 15:29:31.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.5042 test 0.4814 metric ['0.8275']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 128/128 [00:00<00:00, 141.04it/s]\n",
      "\u001b[32m2025-09-26 15:29:32.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.4805 test 0.4679 metric ['0.8302']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=5,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=128,\n",
    "    valid_steps=128,\n",
    "    reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "\n",
    "units = [256, 128, 64]\n",
    "for _ in range(5):  \n",
    "    with mlflow.start_run():\n",
    "        trainstreamer = train.stream()\n",
    "        validstreamer = valid.stream()\n",
    "        unit1 = random.choice(units)\n",
    "        unit2 = random.choice(units)\n",
    "        mlflow.set_tag(\"model\", \"convnet + batchnorm\")\n",
    "        mlflow.set_tag(\"dev\", \"tyco\")\n",
    "        mlflow.log_params({\n",
    "                            \"epochs\": settings.epochs,\n",
    "                            \"train_steps\": settings.train_steps,\n",
    "                            \"valid_steps\": settings.valid_steps,\n",
    "                            \"logdir\": settings.logdir,\n",
    "                            \"reporttypes\": [r.name for r in settings.reporttypes],  \n",
    "                        })\n",
    "        mlflow.log_param(\"units1\", unit1)\n",
    "        mlflow.log_param(\"units2\", unit2)\n",
    "        mlflow.log_param(\"batchsize\", f\"{batchsize}\")\n",
    "        model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        modelpath = modeldir / (tag + \"model.pt\")\n",
    "        torch.save(model, modelpath)\n",
    "        mlflow.log_artifact(local_path=modelpath, artifact_path=\"pytorch_models\")\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b9838",
   "metadata": {},
   "source": [
    "Save model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d18dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomlserializer = TOMLSerializer()\n",
    "tomlserializer.save(settings, \"settings.toml\")\n",
    "tomlserializer.save(model, \"model.toml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
