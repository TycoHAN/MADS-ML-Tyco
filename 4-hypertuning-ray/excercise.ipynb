{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38738c5b",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b1fcf8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import ReportTypes, Trainer, TrainerSettings, metrics, rnn_models\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fd3dc",
   "metadata": {},
   "source": [
    "Setting seeds for isolated testing, but doesnt fix all randomness unfortunately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1445640",
   "metadata": {},
   "source": [
    "Get flowers data into a streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "705d3ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-26 14:24:04.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\tycoh\\.cache\\mads_datasets\\flowers\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "flowers = DatasetFactoryProvider.create_factory(DatasetType.FLOWERS)\n",
    "streamers = flowers.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers['train']\n",
    "valid = streamers['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b960b7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec277b66",
   "metadata": {},
   "source": [
    "Create a configurable model that can be hypertuned for the flowers dataset classification\n",
    "\n",
    "Show you can\n",
    "1. Make a hypothesis based on the theory (use the book)\n",
    "1. Design experiments to test your hypothesis\n",
    "1. Work iterative: eg start with a small experiment to get a direction, then reduce the search space and run a more extensive experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc20478",
   "metadata": {},
   "source": [
    "For classifying flowers we need a convolutional neural network because images are high dimensional, nearby pixels are statistically related and if pictures shifts a little al pixels values are different but it is still the same picture. By using a convolutional neural network we make sure we can use weight sharing to deal with the high dimensions, the kernel also takes care of nearby related pixels and takes care of recognizing the geomtric transformations. There are multiple architectures to choose from like LeNet, AlexNet (8 layers), VGG (19 layers), GoogLeNet (22 layers, inception), ResNet(152 layers, skip layers), SqueezeNet (less parameters, 50x less then alexnet). I am working on a simple laptop with cpu so i would like the model which is trained te fastest.   \n",
    "\n",
    "Hypothesis\n",
    "- Increasing the number of batchnorm layers increases the accuracy of the TestCNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca19b0",
   "metadata": {},
   "source": [
    "settings, trainer, ml flow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "60e04742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "# make a CNN class\n",
    "class TestCNN(nn.Module):\n",
    "    # initialise class\n",
    "    def __init__(self, num_classes: int, dropout: float) -> None:\n",
    "        # inherent functions from module\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "\n",
    "                nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "        \n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),       \n",
    "        )\n",
    "        self.agg = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.agg(x)\n",
    "            x = self.classifier(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a1fd6",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "194704b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:c:/Users/tycoh/Desktop/MADS-ML-Tyco/4-hypertuning-ray/mlruns/2', creation_time=1761315160087, experiment_id='2', last_update_time=1761315160087, lifecycle_stage='active', name='exercise_4', tags={}>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "experiment = \"exercise_4\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eb744f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "41692e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-26 15:13:01.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20251026-151301\u001b[0m\n",
      "\u001b[32m2025-10-26 15:13:01.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 50/50 [00:27<00:00,  1.84it/s]\n",
      "\u001b[32m2025-10-26 15:13:41.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 1.5680 test 1.4668 metric ['0.3619']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 50/50 [00:30<00:00,  1.64it/s]\n",
      "\u001b[32m2025-10-26 15:14:25.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.3551 test 1.2149 metric ['0.4713']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 50/50 [00:28<00:00,  1.78it/s]\n",
      "\u001b[32m2025-10-26 15:15:05.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 1.2084 test 1.1259 metric ['0.4894']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 50/50 [00:25<00:00,  1.99it/s]\n",
      "\u001b[32m2025-10-26 15:15:43.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 1.1659 test 1.0951 metric ['0.5519']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 50/50 [00:27<00:00,  1.79it/s]\n",
      "\u001b[32m2025-10-26 15:16:25.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 1.1428 test 1.0514 metric ['0.5678']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [03:23<00:00, 40.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "import torch.optim as optim\n",
    "\n",
    "with mlflow.start_run():\n",
    "    settings = TrainerSettings(\n",
    "        epochs=5,\n",
    "        metrics=[metrics.Accuracy()],\n",
    "        logdir='modellogs',\n",
    "        train_steps= 50,\n",
    "        valid_steps= 50,\n",
    "        reporttypes=[ReportTypes.MLFLOW]\n",
    "    )\n",
    "    mlflow.log_params({\n",
    "        \"epochs\": settings.epochs,\n",
    "        \"metrics\": settings.metrics,\n",
    "        \"train_steps\": settings.train_steps,\n",
    "        \"valid_steps\": settings.valid_steps\n",
    "    })\n",
    "    model = TestCNN(num_classes=5, dropout=0.5)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        settings=settings,\n",
    "        loss_fn=nn.CrossEntropyLoss(),\n",
    "        optimizer= torch.optim.Adam,\n",
    "        traindataloader=trainstreamer,\n",
    "        validdataloader=validstreamer,\n",
    "        scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    )\n",
    "    trainer.loop()\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89727bff",
   "metadata": {},
   "source": [
    "2025-10-26 14:37:05.879 | INFO     | mltrainer.trainer:report:209 - Epoch 0 train 1.5653 test 1.4335 metric ['0.3764']\n",
    "100%|██████████| 45/45 [00:26<00:00,  1.70it/s]\n",
    "2025-10-26 14:37:35.279 | INFO     | mltrainer.trainer:report:209 - Epoch 1 train 1.3375 test 1.2502 metric ['0.4205']\n",
    "100%|██████████| 45/45 [00:25<00:00,  1.73it/s]\n",
    "2025-10-26 14:38:04.149 | INFO     | mltrainer.trainer:report:209 - Epoch 2 train 1.2489 test 1.1953 metric ['0.4616']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677f06f",
   "metadata": {},
   "source": [
    "Hypothesis\n",
    "- Model has an increase in accuracy by using dropout regularization because this makes the model less dependent on any given hidden unit and encourages weights to have smaller magnitudes and therefore generalize better\n",
    "\n",
    "Experiment\n",
    "- We set a seed so it is an isolated experiment with na randomness\n",
    "- First, we run 3 epochs with TestCNN to examine accuracy with dropout set to 0.0\n",
    "- Next, we run 3 epochs with TestCNN with dropout set to 0.2 and examine the results\n",
    "- Lastly, we run 3 epochs with TestCNN with dropout set to 0.6 and examine the results\n",
    "\n",
    "Results\n",
    "- Dropout 0.0 gives max accuracy 0.48\n",
    "- Dropout 0.5 gives max accuracy 0.46\n",
    "- Dropout 1.0 gives max accuracy 0.21\n",
    "\n",
    "Conclusion\n",
    "- Without dropout the model has the best performance. This rejects the hypothesis that it should generalize better with dropout. I believe this is a bad experiment, because this does not match with the theory (which has been tested as well).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c55d44",
   "metadata": {},
   "source": [
    "Possible experiments:\n",
    "- timeseries with cnn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
